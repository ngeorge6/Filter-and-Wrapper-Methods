{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wrapper_Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hDk9hDvOH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "769f26a5-50fe-46d6-b9ea-cbb380f04ce2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#NLTK-------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stemporter import PorterStemmer\n",
        "\n",
        "# Import libraries for feature \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#Change current working directory to gdrive\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMwGQK7KAd7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "64bdbec9-9ce5-4597-d7b1-2fd65f32ef8d"
      },
      "source": [
        "#Read files\n",
        "textfile = r'/gdrive/My Drive/CIS 508/Assignment/HW5/Comments.csv'\n",
        "textData = pd.read_csv(textfile) #creates a dataframe\n",
        "\n",
        "CustInfofile = r'/gdrive/My Drive/CIS 508/Assignment/HW5/Customers.csv'\n",
        "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
        "\n",
        "print(textData.shape)\n",
        "print(CustInfoData.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 2)\n",
            "(2070, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWOTk6C1Ao45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "be1e40e7-3b4b-431e-de5a-8461f3559431"
      },
      "source": [
        "#Extract target column from Customer Info file\n",
        "y_train = CustInfoData[\"TARGET\"]\n",
        "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "                     \n",
        "print(X_train.shape)\n",
        "print(textData.shape)\n",
        "textData.head()\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 16)\n",
            "(2070, 2)\n",
            "0       Cancelled\n",
            "1         Current\n",
            "2         Current\n",
            "3         Current\n",
            "4       Cancelled\n",
            "          ...    \n",
            "2065    Cancelled\n",
            "2066    Cancelled\n",
            "2067    Cancelled\n",
            "2068    Cancelled\n",
            "2069    Cancelled\n",
            "Name: TARGET, Length: 2070, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWYNz2Ep17l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "newTextData=pd.DataFrame()\n",
        "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "#Join stemmed strings\n",
        "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS 508/Assignment/HW5/newTextData-Joined.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBguQloljam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "327fcda9-4c12-48b2-cb94-d522cfdc514f"
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
        "print(TD_counts.shape)\n",
        "TD_counts.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "#print(TD_counts)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
        "#print(DF_TD_Counts)\n",
        "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/CIS 508/Assignment/HW5/TD_counts-TokenizedStemmed.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd8TZYnAxQbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "ffdf790d-11a8-4231-b433-bf3e82028f40"
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts)\n",
        "print(X_train_tfidf.shape)\n",
        "DF_TF_IDF=pd.DataFrame(X_train_tfidf.toarray())\n",
        "print(DF_TF_IDF)\n",
        "export_csv= DF_TF_IDF.to_csv(r'/gdrive/My Drive/CIS 508/Assignment/HW5/TFIDF_counts-TokenizedStemmed.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "      0    1    2    3        4    5    ...  348  349  350  351  352  353\n",
            "0     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.27568  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...  ...      ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GciMzMrgKNU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "1e3633c7-0789-4a37-a426-279d68972ea2"
      },
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "\n",
        "sfs1.fit(X_train_tfidf,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 354 out of 354 | elapsed:   28.0s finished\n",
            "\n",
            "[2019-12-13 16:04:45] Features: 1/5 -- score: 0.6164312314704302[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 353 out of 353 | elapsed:   28.8s finished\n",
            "\n",
            "[2019-12-13 16:05:14] Features: 2/5 -- score: 0.6193216167473699[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 352 out of 352 | elapsed:   29.2s finished\n",
            "\n",
            "[2019-12-13 16:05:43] Features: 3/5 -- score: 0.6227055986869893[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 351 out of 351 | elapsed:   29.5s finished\n",
            "\n",
            "[2019-12-13 16:06:12] Features: 4/5 -- score: 0.624152534621766[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 350 out of 350 | elapsed:   29.8s finished\n",
            "\n",
            "[2019-12-13 16:06:42] Features: 5/5 -- score: 0.6260860714852955"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialFeatureSelector(clone_estimator=True, cv=5,\n",
              "                          estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                           class_weight=None,\n",
              "                                                           criterion='gini',\n",
              "                                                           max_depth=None,\n",
              "                                                           max_features='auto',\n",
              "                                                           max_leaf_nodes=None,\n",
              "                                                           min_impurity_decrease=0.0,\n",
              "                                                           min_impurity_split=None,\n",
              "                                                           min_samples_leaf=1,\n",
              "                                                           min_samples_split=2,\n",
              "                                                           min_weight_fraction_leaf=0.0,\n",
              "                                                           n_estimators='warn',\n",
              "                                                           n_jobs=None,\n",
              "                                                           oob_score=False,\n",
              "                                                           random_state=None,\n",
              "                                                           verbose=0,\n",
              "                                                           warm_start=False),\n",
              "                          floating=False, forward=True, k_features=5, n_jobs=1,\n",
              "                          pre_dispatch='2*n_jobs', scoring='accuracy',\n",
              "                          verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40NUcNi_LTv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e412b82-f2e9-45d1-ba4c-6bfec6ef3f4d"
      },
      "source": [
        "feat_cols = list(sfs1.k_feature_idx_)\n",
        "print(feat_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[55, 141, 155, 295, 342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1DNADPVLssr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c45e37e7-f9d8-423c-ca7a-de3f71c093a1"
      },
      "source": [
        "new_data = DF_TF_IDF[[14,24,155,295,342]]\n",
        "print(new_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrSDlSm-OLHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "15e9c061-f5a6-465e-8d24-552def5aefbe"
      },
      "source": [
        "combined=pd.concat([X_train, new_data], axis=1)\n",
        "print(combined.shape)\n",
        "print(combined)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 21)\n",
            "        ID Sex Status  Children  Est_Income  ...   14   24  155  295  342\n",
            "0        1   F      S         1    38000.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "1        6   M      M         2    29616.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2        8   M      M         0    19732.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "3       11   M      S         2       96.33  ...  0.0  0.0  0.0  0.0  0.0\n",
            "4       14   F      M         2    52004.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "...    ...  ..    ...       ...         ...  ...  ...  ...  ...  ...  ...\n",
            "2065  3821   F      S         0    78851.30  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2066  3822   F      S         1    17540.70  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2067  3823   F      M         0    83891.90  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2068  3824   F      M         2    28220.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2069  3825   F      S         0    28589.10  ...  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 21 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg4MWYX-OW6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bb1798fd-4e0d-4270-9249-f230f0f395dd"
      },
      "source": [
        "X_cat = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
        "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
        "print(X_cat)\n",
        "combined_one_hot = pd.get_dummies(combined,columns=X_cat)\n",
        "print(combined_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype']\n",
            "(2070, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ad9AWyPOmf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6e4f2fbf-e924-4c61-e5fa-6942da2f94a6"
      },
      "source": [
        "X = combined_one_hot\n",
        "y_train = CustInfoData[\"TARGET\"]\n",
        "\n",
        "print (X.shape)\n",
        "print (y_train.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y_train, test_size = 0.2, random_state= 42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 29)\n",
            "(2070,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKkhWALhOpcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "42506830-6eac-46eb-92f8-750d24ccac58"
      },
      "source": [
        "RF_Comb = clf.fit(X_train,y_train)\n",
        "# print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_train, y_train)))\n",
        "rf_predictions = clf.predict(X_test)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, rf_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.855072\n",
            "Confusion Matrix:\n",
            "[[128  29]\n",
            " [ 31 226]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.81      0.82      0.81       157\n",
            "     Current       0.89      0.88      0.88       257\n",
            "\n",
            "    accuracy                           0.86       414\n",
            "   macro avg       0.85      0.85      0.85       414\n",
            "weighted avg       0.86      0.86      0.86       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}